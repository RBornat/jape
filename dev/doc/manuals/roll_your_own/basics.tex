% !TEX root = roll_your_own.tex
\chapter{Basic Principles}
\label{chap:basics}

Jape works by applying inference rules to sequents in proof trees. Its fundamental mechanism is unification, laced with a pragmatic treatment of explicit substitution forms. We decided on unification rather than one-way pattern-matching because it allows us to use Jape as a Prolog-style calculator, solving question-problems such as
\begin{quote}
$\lambda x.\lambda y.(x\;y): \_T$
\end{quote}
which would be completely intractable, or pointless, in a one-way-matching engine.

Tactics in Jape organise the application of other tactics. The simplest tactic is an inference rule.

On top of its basic proof mechanism Jape provides you with the opportunity to control the graphical user interface by programming its response to the basic gestures of pointing and clicking, and by defining what is included in the menus and panels shown to the user.

\section{Flexible syntax}

Jape has a built-in collection of syntactic forms which you can customise and to which you can add the particular details which are appropriate to your particular logic. It recognises numbers, strings, identifiers, unknowns, bracketed formulae, tuples, substitutions, juxtapositions, and formulae made by using user-defined prefix, postfix and infix operators, with user-defined priorities and associativity. In addition you can invent various new kinds of brackets and punctuation.

Identifiers --- names like $A$, $x$ or $F$ in conjectures, theorems and rules --- rarely stand for themselves. For the most part they stand for some arbitrary formula, variable or predicate which can appear in an instance of the conjecture, theorem or rule in which they are used. When you define the syntax of identifiers in your logic you say which are schematic identifiers and which are constants. At the same time you can define the syntactic category of the identifiers you use.

The flexible syntax mechanism is illustrated in every chapter, and detailed in \appxref{paraformlang}.

%\section{Backward proof}

%Jape always, always, always works backwards --- from conclusion towards antecedents --- even when its display mechanism tries to produce the illusion that it is working forwards. It always works with trees --- Gentzen trees --- of sequents, even when its display mechanism is trying to produce the illusion that it is working with Fitch boxes, or something else. Currently you (or the user) can choose between tree display and box-and-line display for proofs, and there is a treatment of transitional reasoning within box-and-line. We have the beginnings of a more attractive treatment of equational chaining proofs (see, for example, \chapref{funcprog}) and we dream of a kind of Fitchery for linear logic, and more.}. If you haven't seen a Gentzen tree, you can learn how Jape handles them if you use one of the distributed logics that uses tree display mode, or you can switch to tree display mode in one of the distributed logics that allows it --- for example the single-conclusion sequent calculus encoding.

\section{Inference rule matching}

Consider an example rule of the single-conclusion sequent calculus:
\begin{quote}
$\infer[\reason{∨⊦}]
       {\Gamma,A|B |- C}
       {\Gamma,A |- C & \Gamma,B |- C}$
\end{quote}

Its rendition in Japeish (see the file SCS\_rules.j) is a straightforward linearisation.
\begin{quote}
\tt RULE "∨⊦" FROM Γ, A ⊦ C AND Γ, B ⊦ C INFER Γ, A∨B ⊦ C
\end{quote}

Jape's interpretation of the rule is as a description of a node in a proof tree by pattern: the consequent at that node has a collection of formulae on its left-hand side, one of which matches $A|B$ for some pair of formulae $A$ and $B$ and the rest of which are taken as $\Gamma$, and a single right-hand side formula which matches $C$. The node has two antecedents, each of which contains a sequent with the same right-hand side formula $C$. The left-hand antecedent will have a sequent whose left-hand side is $\Gamma$ together with the formula $A$; the right-hand antecedent's left-hand side is $\Gamma$ together with $B$.

Jape makes proofs by unifying (i.e. matching) consequents of rules to tips of the tree, and replacing the tip with the corresponding node. With the exception of \textsc{resolve} and \textsc{cutin} steps, described later, that's \emph{all} it does. Bernard and I, like many before us, fixed on that simple mechanism because we thought we had a chance of getting it right. It means that Jape only works backwards --- from consequent to antecedents --- even when it seems not to.

The single rule above contains lots of different kinds of symbol. There are the schematic identifiers $\Gamma$, $A$, $B$ and $C$: there is the connective $|$; there are the punctuation marks ⊦ and comma; there's the (quoted) rule name ∨⊦; and there are the reserved words of Japeish \textsc{rule}, \textsc{from}, \textsc{and} and \textsc{infer}. Not all the logical identifiers are schematic: $|$ is in a sense an identifier, but it plays a fixed syntactic r\^{o}le in the logic and in the rule. Apart from the connectives, other identifiers might be non-schematic: there could be constant identifiers $\<true>$ and $\<false>$, for example. As logic describer you have control over the matter, which you exercise by organising identifiers into syntactic categories. This not only allows you to distinguish between schematic and other identifiers, but it also allows you to distinguish, for example, between names like $A$ which might be taken to stand for some arbitrary formula, and names like $x$ which you might wish to stand only for variables.

\textit{Parameters of rules}
\label{sec:basics:parameters}

In simple cases the fact that Jape uses unification rather than one-way pattern matching doesn't have a visible effect on the course of a proof. But if a rule doesn't have the subformula property --- if there are names in its antecedents that don't appear in its consequent, as for example in
\begin{quote}
$\infer[\reason{$!-I$}]
       {\Gamma  |- !A}
       {\Gamma,A |- B@!B}$
\end{quote}
--- then \emph{unknowns} may appear in the proof in place of the schematic identifier $B$.\footnote{This is a \textit{strength} of Jape, not a weakness: we don't require the user to decide prematurely on the identity of those unknowns. As a logic designer, however, you can decide not to let your users see very many unknowns: see \chapref{I2L}, for example}

In these and other circumstances it can be useful to allow the user to provide an argument formula which modifies the instantiation step. You do that by writing the rule definition with a parameter. The rule above is written in Japeish as
\begin{quote}
\tt RULE "¬-I"(B) IS FROM Γ,A ⊦ B∧¬B INFER Γ ⊦ ¬A
\end{quote}
Given a problem sequent $E,F->!E |- !F$, Jape first instantiates the rule by replacing all the schematics with corresponding unknowns; then it unifies \_Γ with $E,F->!E$ and $\_A$ with $F$; thus it generates the antecedent $E,F->!E,F |- \_B@!\_B$ . If it is given the argument formula $E$ to unify with $\_B$, it will generate the antecedent $E,F->!E,F |- E@!E$ . In many cases an argument supplied to the application of a rule can prevent a startling proliferation of unknowns in a proof.

The parameter of a rule may in some circumstances be decorated with the word \textsc{object}. That indicates that in the absence of a user-supplied argument, the instantiation step is to generate a freshly-minted identifier in its place rather than a fresh unknown. Frequently this is because the rule expresses a generalisation step in the logic and it is natural for Jape to mint a fresh name. For example, the rule
\begin{quote}
$\infer[\reason{\textsc{(fresh }$c$, $c$ \textsc{notin} $|*x.A$) ∃-E}]
       {\Gamma |- C}
       {\Gamma  |-|*x.A & \Gamma,A[x\backslash c] |- C}$
\end{quote}
is written as
\begin{quote}
\tt RULE "∃-E"(OBJECT c) WHERE FRESH c AND c NOTIN ∃x.A IS \\
\tab FROM Γ ⊦ ∃x.A AND Γ,A[x\textbackslash c] ⊦ C INFER Γ⊦C
\end{quote}
\textsc{Object} parameters can be used in other circumstances --- in particular, see the discussion of substitution unification below.

\section{Explicit provisos}
\label{sec:basics:provisos}

Lots of rules have side conditions: the ∃-E rule above, for example, has two. There are traditional ways of dealing with these in proof machinery. Jape takes the simplistic route and includes explicit, visible \emph{provisos} in the proof. Jape's provisos at present are \textsc{notin} and \textsc{unifieswith}, three macro-relatives of \textsc{notin} --\textsc{fresh, hypfresh} and \textsc{concfresh} -- and three kinds of discharge -- \textsc{discharge}, \textsc{ntdischarge} and \textsc{sntdischarge}.

Provisos are either satisfied or violated, and they constrain the application of rules. If an attempted application would violate a proviso, whether one contained in the rule itself or one left over from an earlier stage of the proof, then the attempt fails. If it is impossible to determine the status of a proviso, because it contains unknowns and/or substitution forms, then it is stored, displayed in a pane below the proof, and carried forward in the expectation that its status will become clearer.

The proviso $x$ \textsc{notin} $E$ is satisfied if $x$ doesn't appear free in $E$, and violated if it does --- or rather, it is satisfied if $x$ \emph{cannot} appear free in $E$, no matter what future unifications may happen and no matter how the schematic identifiers of the conjecture being proved are instantiated. The proviso $x$ \textsc{notin} $y$, for example, is not trivially satisfied if $x$ and $y$ are schematic parameters of a  conjecture being proved, even though visibly $y$ is not the same variable as $x$.

\textsc{notin} provisos are either included in the statement of a rule or generated from \textsc{fresh, hypfresh} or \textsc{concfresh} provisos: \textsc{fresh} $x$ generates a proviso $x$ \textsc{notin} $E$ for every left- and right-hand side formula $E$ of the sequent matching the rule; \textsc{hypfresh} $x$ generates \textsc{notin} provisos only for the left-hand side formulae and \textsc{concfresh} for the right-hand side formulae.

\var{E1,E2}
The proviso $\<E1>$ \textsc{unifieswith} $\<E2>$ is internally generated. It allows Jape to defer difficult unifications where it can't find a most-general unifier. This can arise because of difficulties in unifying substitution forms, or when using multiplicative (context-splitting) rules. It plays a r\^{o}le in Jape's drag-and-drop mechanism for resolving context splits: see below.

The proviso \textsc{discharge} $E$, where is a hypothesis introduced by a rule or a theorem, is satisfied if $E$ is discharged (used in a rule). It's violated if there is no way the proof can progress so that $E$ can be discharged. The variant \textsc{ntdischarge} $E$ is also violated if $E$ is trivially or immediately discharged (\textsc{nt} for non-trivial). \textsc{sntdischarge} $E$ is also violated if $E$ is trivially or multiply discharged (\textsc{s} for single).

\subsection{Counting discharges}

An hypothesis is discharged when it's used as an argument in a rule, broadly speaking. But the structural rules (cut, left weaken, etc. -- see \secref{basics:application}) somewhat complicate things. First, if a hypothesis is used in a left weaken rule it isn't discharged, it's discarded. Second, if a hypotheses is discharged in the left subtree of a cut, then it is implicitly discharged when the conclusion of the left-hand subtree is discharged as a hypothesis in the right subtree.

The first difficulty can be avoided by declaring a left weaken rule to Jape, if your encoding has one (see \secref{paraformlang:structurerules}). Otherwise, Jape may count a discard as a discharge.

The second difficulty is, currently, evaded. Discharge-counting only matters, and then only debatably, in the Syllogisms encoding (see \chapref{Aristotle}). Only explicit discharges are counted.

\section{Conjectures and theorems}

We allow the user to state a conjecture using the \textsc{theorem} directive.\footnote{Bernard and I had rows over this notation. I wanted to emphasise that it's a conjecture till it's proved; he wanted to emphasise that it is theorems, after all, that you are trying to prove. I still think I was righter than he was.} A proved conjecture becomes a theorem and can then be applied as a derived rule. If the state variable \texttt{applyconjectures} is set to theorems, or to all, then unproved conjectures can be applied as well.

The \textsc{theorem} directive gives the name of a conjecture and its sequent, and it may also include provisos which will be enforced both during the proof of the conjecture and whenever the theorem is applied. It is possible to define a theorem without giving a name, in which case the sequent itself is used as the name. The \textsc{theorems} directive allows you to state a collection of conjectures, and will give each its own sequent as a name. For example, the SCS.jt file defines a conjecture called \textit{contradiction}
\begin{quote}
\tt THEOREM contradiction IS A, ¬A ⊦ B
\end{quote}
and the sequent\_problems.j file includes a large collection of conjectures named by their sequent, some of which are as follows:
\begin{quote}\tt
THEOREMS PropositionalProblems ARE\\
\tab P→(Q→R) ⊦ (P→Q)→(P→R)\\
AND\tab P→(Q→R), Q ⊦ P→R\\
\tab ....\\
AND\tab WHERE x NOTIN P INFER P ∧ ¬P, ∀x.P→Q, ∀x. ¬P→Q ⊦ ∀x.Q \\
AND\tab R ∧ ¬R, ∀x.R→S, ∀x. ¬R→S ⊦ ∀x.S\\
....\\
AND\tab ∃y.P ⊦ ∀y.P\\
END
\end{quote}
The conjectures in this illustration are called ``P→(Q→R) ⊦ (P→Q)→(P→R)'', ``P→(Q→R), Q ⊦ P→R'', ``P∧¬P, ∀x.P→Q, ∀x.¬P→Q ⊦ ∀x.Q'' (the provisos aren't part of the name), ``R∧ R, ∀x.R→S, ∀x.¬R→S ⊦ ∀x.S'' and ``∃y.P ⊦ ∀y.P''.

\subsection{Proving a conjecture --- substitutions and provisos}

A proof of a conjecture begins with a tree which consists of the base sequent of the conjecture, together with any provisos which were included in the statement of the conjecture\footnote{Jape sometimes adds invisible provisos which it deduces from the binding structure of the formulae in the conjecture: see below, and also \appxref{GUIlang}.}. The proof is then developed by application of rules and tactics.

One important feature of the proof is Jape's treatment of the identifiers and unknowns that appear in the base sequent of the conjecture, and its treatment of other identifiers that may be introduced during the proof process. Jape's theorems are theorem schemata, not particular theorem formulae; they may therefore be instantiated in the same way as a rule, replacing identifiers in the theorem sequent by unknowns or arbitrary formulae. Identifiers in the conjecture's sequent can't, therefore, be treated as standing for themselves during the proof.

In practice this means that substitution forms involving those identifiers may not be simplifiable: if, for example, identifiers $A$ and $x$ appear in the conjectured sequent then $A[x\backslash E]$ can't be replaced by $A$ unless it is certain that there will never be an instance of the theorem in which the formula which instantiates $A$ has a free occurrence of the variable which instantiates $x$. But if $x$ is a name introduced during the proof --- for example, by application of a rule which has a \textsc{fresh} proviso --- and if there aren't any unknowns remaining which would allow $x$ to be smuggled into the sequent finally proved, then we reason that whatever argument formula instantiates $A$, we could choose $x$ within the proof to be distinct from all the names in $A$, and therefore $A[x\backslash E]$ can be replaced by $A$. In other circumstances the assurance that $x$ can't occur free in $A$ can come from a \textsc{notin} proviso, or from meta-theoretical reasoning about the relationships of names in the conjectured sequent.

Provisos that are introduced during proof of a conjecture, by application of rules or other theorems or conjectures, and which aren't evidently satisfied or violated are retained as part of the theorem and checked whenever the theorem is applied.

The effect of our care with substitutions and provisos is that the proof tree which establishes the validity of a conjecture stands for all the proof trees of all the instances of that conjecture, and Jape is justified in using such a conjecture as a derived rule.

\subsection{Applying a theorem: the r\^{o}le of structural rules}
\label{sec:basics:application}
(\emph{This section is confused and possibly nonsense. I'm working on it. RB 2020/10/26})

A theorem is, in principle, a rule with no antecedents. So Jape can instantiate it as a rule and match it to a problem sequent just as a rule is matched. There are, however, a couple of interesting points.

The first is that in many cases a theorem won't have enough left-hand or right-hand side formulae to completely match a problem sequent. The theorem ``P→(Q→R) ⊦ (P→Q)→(P→R)'', for example, matches only sequents with exactly one formula on the left of the turnstile and one on the right. Often a logic will include so-called `weakening' rules which enable you to delete a formula from the left- or the right-hand side or both. If you include such rules and declare their r\^{o}le to Jape, it will allow you to apply a theorem even though it does not completely match a problem sequent.

The second difficulty is that sometimes a theorem matches on the right-hand side, but not on the left. In such a case it is often convenient to prove it `by resolution': that is, to generate an antecedent for each of the left-hand side formulae and to set about proving them. That step is justified if the logic contains a `cut' rule which enables you to move formulae from left- to right-hand side and a `weaken' rule as well. Jape will make a resolution step for you if you declare the appropriate structural rules in your logic, declare their r\^{o}les, and also set the \texttt{tryresolution} variable (see \appxref{GUIlang}) or use one of the \textsc{applyorresolve} or \textsc{resolve} tacticals (see \appxref{tacticlang}).

\section{Substitution forms and unification}

Jape uses explicit substitution forms --- $A[x\backslash c]$, $B[x,y,z\backslash E,F,G]$ --- where some logics use predicate notation --- $P(c)$, $Q(E,F,G)$ . Substitutions are more powerful than predicates because they are more general; for the same reason they are trickier to handle. Jape's internal mechanisms are based on substitution forms, but there is also a mechanism which allows you to write rules and theorems in terms of predicate formulae --- see `interpreting predicates' below.

Explicit substitution forms are semantically scandalous, a notorious trap for novices, and an expert will ask ``what does a substitution form in a rule or theorem \emph{mean}?''. It's difficult to give a simple answer. It is never necessary to include special rules to treat substitution forms --- their treatment is a fundamental mechanism of Jape, and Jape tries to eliminate substitution forms from the proof whereever and whenever they appear. Therefore we can say that Jape treats a substitution form as equivalent to the result of carrying out the subsitution. But in some situations it can be persuaded to treat a substitution form as a structural pattern and will unify one unreduced substitution form with another, even though such unifications don't give the most general answer.

A substitution form is introduced into a proof, and if possible immediately eliminated, whenever the antecedent of a rule contains one. Consider, for example, the problem sequent $x>y |-|*z.z>y$ . If we apply the rule
\begin{quote}
$\infer[|-\exists] {\Gamma|-|*x.A,\Delta} {\Gamma|-A[x\backslash E],\Delta}$
\end{quote}
then we generate a single antecedent $x>y |- (z>y)[z\backslash \_E]$, which immediately simplifies to $x>y |- \_E>y$ provided that we know that $z$ and $y$ are necessarily distinct,\footnote{They might not be if, for example, they both appear in the base sequent of the conjecture being proved. They may be if, for example, one or the other has been generated during the development of the proof, or if there is an explicit proviso which makes it clear that they are distinct.} or to $x>y |- \_E>y[z\backslash \_E]$ otherwise.

Even more interesting is what happens when a rule contains an explicit substitution form such as $A[x\backslash E]$ in its \emph{consequent}. When the rule is applied Jape must unify that substitution form --- or rather, its instantiated form which in general will be $\_A[\_x\backslash \_E]$ --- with some formula $B$ in the problem sequent. That sort of unification is notoriously difficult, and Jape uses a number of ad-hoc strategies to help.
\begin{enumerate}
\item It simplifies substitution forms whenever possible, in order to avoid the problem.
\item It defers the unification of an irreducible substitution form for as long as possible, so that the results of other unifications can be used to simplify it.
\item If the user provides an argument formula \textit{F} in place of parameter $E$, the instantiated form will be $\_A[\_x\backslash F]$ ; when Jape can no longer avoid unifying that form with $B$ it will search for all instances of \textit{F} inside $B$ and try to construct a substitution form $B' [\_x\backslash F]$ which simplifies to $B$ in presence of the proviso $\_x$ \textsc{notin }$B$; if successful it will unify $\_A$ with $B'$ in a context that records the proviso. The process is far more effective if the parameter $x$ is decorated with the word \textsc{object}, so that the instantiated form becomes $\_A[z\backslash F]$ where $z$ is a fresh variable; the formula $B' [z\backslash F]$ is easier to construct and to simplify, and the proviso $z$ \textsc{notin} $B$ is easier to check\footnote{In fact, because $z$ is a fresh variable, the proviso is usually obviously satisfied. But a proviso is necesssary to constrain the future course of the proof if there are unknowns in $B$. In those and in some other circumstances Jape may also produce \textsc{unifieswith} provisos to cater with the process of abstraction in the nasty bits of $B$.}.
\item If the user text-selects instances of a sub-formula \textit{F} of $B$, then the logic encoding can employ the \textsc{withsubstsel} tactical --- see \chapref{funcprog} and \appxref{tacticlang} --- to calculate $B'$ by replacing just those instances of \textit{F} by a fresh unknown $\_y$; $B' [\_y\backslash F]$ necessarily simplifies to $B$ given the proviso $\_y$ \textsc{notin} $B$; then Jape will match the substitutions structurally, unifying $\_A$ with $B'$, $\_x$ with $\_y$ and $\_E$ with \textit{F} in a context which records the proviso. If the parameter $x$ is decorated with the word \textsc{object} then $\_x$ is replaced by $z$ and the proviso becomes $z$ \textsc{notin} $B$, which is once again easier to check.
\item If all else fails, Jape can generate a proviso $\_A[\_x\backslash \_E]$ \textsc{unifieswith} $B$, and await developments.
\end{enumerate}


If Jape has to unify two substitution forms which have identical variable lists then it matches them structurally. For example, it can unify $A[x,y\backslash A1,A2]$ with $B[x,y\backslash B1,B2]$ by unifying $A$ with $B$, \textit{A1} with \textit{B1}, \textit{A2} with \textit{B2}. This happens rarely and because it doesn't generate the most general unifier it might not be the best thing to do, but pragmatically it seems to work rather well almost every time it is used.

If Jape has to unify substitution forms with different variable lists then it extends one or the other: for example, if it has to unify $A[x\backslash A1]$ with $B[x,y\backslash B1,B2]$ it will try to construct $A'$ such that $A[y\backslash B2]$ simplifies to $A$, and then unify $A[x,y\backslash A1,B2]$ with $B[x,y\backslash B1,B2]$ . In certain circumstances it will even do a bit of α-conversion --- but enough! this explanation is sufficiently complicated already.

The message is that Jape's unification of substitution forms is usefully pragmatic. It does not always generate a most-general unifier but it can, in practice, often generate just the unifier that the user is looking for, especially when the encoding uses the \textsc{letsubstsel}/\textsc{withsubstsel} mechanism (see \chapref{funcprog} and \appxref{tacticlang}) to allow the user to describe the unification to Jape. It most often breaks down when it has to deal with substitutions using variables which also appear in the base sequent of the theorem being proved. That breakdown is, I think, inevitable, (and investigations stopped some time ago).

\subsection{Invisible provisos}

Consider the conjectures $\lambda x.\lambda y.x:T1->T2->T1$ and $@*x.|*y.P(x)=P(y)$ . Clearly, in each case, any instance of the conjecture would have to use two distinct variables. Nothing else would give the right binding structure: $\lambda z.\lambda z.z:\operatorname{int} ->\operatorname{real}->\operatorname{int} $ isn't an instance of the first conjecture, nor $@*z.|*z.Q(z)=Q(z)$ of the second\footnote{Strictly speaking, this second \textit{might} be an instance of the the conjecture, if there are no instances of $z$ in $Q$. These are deep waters...}. But Jape's mechanisms of rule and theorem instantiation don't automatically ensure this: instead, there has to be a proviso such as $x$ \textsc{notin} $y$ in each case. Such provisos are fussy, have to do with the internal mechanisms of Jape, and are difficult to explain to Jape's users. Therefore Jape generates them automatically, from an analysis of the binding structure of every rule and conjecture, and then makes them invisible. You can see the invisible provisos in a proof by setting the \texttt{showallprovisos} variable to $\<true>$.

\subsection{Interpreting predicate notation}

Some of our users prefer predicate notation to substitution, and in certain ways it concisely conveys more information. In the formula $@*x.P(x)$ it is implicit that the predicate formula $P$ doesn't contain any instances of $x$; in the corresponding formula $@*x.P[v\backslash x]$ no such inference can be drawn, and the statement of a theorem which contained such a formula would require a proviso $x$ \textsc{notin} $P$ to say as much as the predicate version. Fussy provisos get substitution notation a bad name, so we have implemented a mechanism which interprets predicate notation, translating it into substitution notation and inserting invisible provisos.\footnote{Those invisible provisos constrain the proof. If they were too constraining, the worst that could happen is that you miss some proofs.} When you apply a rule which contains $@*x.P(x)$, for example, Jape translates it into $@*x.P[v\backslash x]$, automatically inserting $x$ \textsc{notin} $P$. When you begin a proof which contains $@*x.P(x)$, Jape doesn't translate it, but it does insert the same proviso, making it invisible.

If you set the variable \texttt{interpretpredicates} to $\<true>$, Jape treats every juxtaposition as if it were a predicate application. If \texttt{interpretpredicates} is $\<false>$ (the default), Jape only interprets those juxtapositions in which the first formula is an \textsc{abstraction} parameter name. See chapters \ref{chap:ItL} and \ref{chap:funcprog} for examples.

In one respect Jape's interpretation of predicate notation is pragmatically helpful rather than careful. Consider, for example, the sequent $|*x.@*y.P(x,y) |- @*y.|*x.P(x,y)$ . Jape translates this to $|*x.@*y.P[u,v\backslash x,y] |- @*y.|*x.P[u,v\backslash x,y]$, and automatically includes provisos $x$ \textsc{notin} $P$ and $y$ \textsc{notin} $P$, as it should. Jape also includes $x$ \textsc{notin} $y$, which isn't essential in order to preserve the binding structure, because it is not required that either $x$ or $y$ must appear free in a predicate $P(x,y)$ . The effect is that certain instances of the theorem are excluded. In practice it seems that our users prefer it this way.

\section{Binding forms: unification, α-conversion and substitution}

Suppose that $@*var.formula$ has been defined to be a binding form: then Jape will proceed as follows:
\begin{itemize}
\item it will unify $@*x.A$ with $@*x.B$ by unifying $A$ with $B$;
\item it will unify $@*x.A$ with $@*\_y.B$ by unifying $x$ with $\_y$ and $A$ with $B$
\item it will unify $@*x.A$ with $@*y.B$ by unifying $@*z.A[x\backslash z]$ with $@*z.B[y\backslash z]$, where $z$ is a fresh variable, together with the provisos $z$ \textsc{notin} $A$ and $z$ \textsc{notin} $B$.
\end{itemize}

Jape respects binding forms when carrying out substitutions. Thus, for example, if $@*var.formula$ has been defined to be a binding form and $x$ and $y$ are guaranteed distinct then $(@*x.A)[x\backslash E]$ always simplifies to $@*x.A$ and, provided that $x$ doesn't appear free in \textit{F}, $(@*x.A)[y\backslash F]$ will simplify to $@*x.(A[y\backslash F])$; in other circumstances it will simplify to by α-conversion to $@*z.(A[x,y\backslash z,F])$ together with the proviso $z$ \textsc{notin} $A$, where $z$ is a fresh variable.

\section{The tactic language}

Although Jape's basic operation is the application of rules to tips of a proof tree, that is by no means the whole story. You will often find it necessary to organise the application of rules by writing programs in the tactic language.

Bernard originally invented the tactic language\footnote{Over my dead body, several times. This is one battle I'm glad I lost.} to enable us to express programmed actions like proof searches. It's become a workhorse for many more tasks than that.

The simplest tactics are inference rules. You can apply tactics sequentially (\textsc{seq}), or try one after another (\textsc{alt, when}), you can call tactics with arguments, you can repeat tactics (\textsc{do}); there is a notion of the `current goal' sequent in the tree which is used when tactics are applied in sequence. It is possible, under very severe constraints, to transform formulae within the goal sequent (\textsc{find, flatten, withsubstsel}).

Most of the language has to do with the interpretation of gestures and selection of an appropriate response.

\Appxref{tacticlang} gives a complete list of all the verbs of the tactic language. The chapters of this manual give examples of their use.

\section{Gestures, menus and panels}
\label{sec:basics:gestures}

The user can make certain `gestures' at the Jape graphical interface. The way in which the gestures are made --- which buttons and keys are pressed and how the mouse is moved --- depends on the operating system. In general a `click' is a left-button mouse click, a `subformula click' is a middle-button click (or a click with the alt/option key held down).
\begin{itemize}
\item A user can \emph{select} a formula in a sequent by clicking it. If a rule is then applied, Jape requires by default that the selected formula is a principal formula in the rule. Thus, for example, if you select the hypothesis $E|F$ in the sequent $G|H, E|F, G->E |- G@E$ and then apply the rule
\begin{quote}
$\infer[\reason{∨⊦}]
       {\Gamma,A|B |- C}
       {\Gamma,A |- C &  \Gamma,B |- C}$
\end{quote}
you ensure that $A|B$ in the rule matches $E|F$ in the sequent, $\Gamma$ matches $G|H,\;G->E$ and, of course, $C$ matches $G@E$. (A tactic can test for formula selection, discover the formula selected, and modify its behaviour accordingly.)
\item A user can double-click (`hit') on a formula, causing the application of a tactic chosen by the logic description.
\item A user can double-click on the `reason' or `justification' of a proof step. If there is hidden detail behind that step then it will be revealed, or if it has been revealed by an earlier double-click, it will be hidden again.
\item A user can drag a formula. If there is a \textsc{unifieswith} proviso, generated as a result of context-splitting in a multiplicative rule, some of the other formulae mentioned in that proviso --- unknown segment variables like \_Γ or \_Δ1 --- will highlight as the formula is dragged across them.
\item A user can \emph{subformula-select} part or all of a formula in a sequent. If a rule is applied, the subformula is provided as the first argument. If a tactic is applied, it can test for subformula selection and modify its behaviour accordingly.
\item A user can select an entry in a menu, and Jape will carry out the corresponding command. Most entries correspond to the command \texttt{apply T} for some tactic \texttt{T}, but a menu can contain any of the commands listed in \appxref{GUIlang}. A good deal of your user-interface design activity will go into deciding what goes in which menus, fixing on labels for each entry and choosing just the right commands.
\item A user can press a button in a panel, with or without first choosing an entry from the list of entries in the same panel. Many panels list conjectures, and their buttons allow users to prove the chosen conjecture, apply it as a theorem and so on. Other panels may be like menus. The designer controls what is in the entries and what is on the buttons, and whether or not a particular button sends just a command, or a command modified by the selected entry.
\item A user can scroll the proof horizontally and/or vertically.
\end{itemize}


And that's it. Jape uses a very impoverished vocabulary of gesture: we have chosen to make it so, in an attempt to make Jape as straightforward to use as any other application in a modern GUI environments.

\section{Proof display: trees, boxes and hiding}

The Gentzen tree is the basic proof structure on which Jape works. Behind the scenes, whatever is on the screen, is a Gentzen tree. Tactics can be used to hide selected antecedents of a proof step and alter the `reason' or `justification' displayed with the step; the hidden detail can be revealed to a user who double-clicks appropriate parts of the proof.

Gentzen trees are notoriously wasteful of display space, and Fitch boxes famously less so. Jape can display a proof in an approximation to Fitch box style. The display is a transcription --- not a translation --- of the tree, and it can be applied to any kind of logic, not simply natural deduction. This is how it's done:
\begin{itemize}
\item the assumptions --- left-hand side formulae --- of the base sequent are written on the first line and the conclusion(s) --- right-hand side formula(e) --- on the last line;
\item if a line is the conclusion of a proof step then the lines representing the trees of its antecedents are written out before it, working left to right through the antecedents;
\item the justification of a line which is the conclusion of a proof step references the assumption line(s) to describe any left-hand side principal formulae, as well as the lines which contain the conclusions of its antecedents;
\item if a line is the conclusion of a tip then a line of dots is written before it;
\item if an antecedent introduces any hypotheses then its lines are written in a box, whose first line is those hypotheses and whose last line is the right-hand side formula(e) of the antecedent.
\end{itemize}


That makes a fairly compact description, in which hypotheses are written only once but conclusions may be written more often, especially when a left-hand side rule is used. It is made still more compact by hiding applications of \textsc{identity} (aka axiom, hypothesis) rules, and it is made to support some forms of forward reasoning (see chapters \ref{chap:ItL} and \ref{chap:I2L}) by hiding, under the right circumstances, applications of a \textsc{cut} rule.

If you select a conclusion formula in a box display, the effect is just as if you had selected the corresponding conclusion formula in the underlying Gentzen tree. If you select a hypothesis formula the effect can't be so simple, because a hypothesis formula is written only once even though it may occur in many sequents: Jape finds the set of sequents that you could be pointing to and disambiguates the choice using any conclusion selection that you might have made.

It doesn't make sense to use box display with a multiple-conclusion calculus for various reasons, and Jape's gesturing mechanisms therefore haven't been adapted to this use.

Our box display isn't a proper Fitch box display because you can't necessarily use the proof which ends on line $j$ when making a proof step on a subsequent line $k$, even though the box structure would allow it. The reason is that line $j$ may be part of the proof of some cousin of $k$, not part of the proof of $k$ --- that is, parts of the proof which are sequentially related in the box display aren't necessarily hierarchically related in the underlying Gentzen tree. There is a tactic-language solution (see \textsc{cutin} in \chapref{I2L}), but without that Jape provides some assistance by making the underlying tree structure more evident when the user selects an assumption or a conclusion: once you make a selection it will `grey out' lines in the box display which are irrelevant because they are not hierarchically related in the underlying tree.

\section{Using Jape interactively}


Jape starts up `empty', with no theory loaded.

You can load a new theory into Jape by using File:Open New Theory. At any time you can add additional bits of Japeish to the brew, by using File:Open. Jape works, like LISP or ML, by maintaining a store of definitions, and it is always possible to add to those definitions. The effects may be strange, especially if you try to add a new theory without getting rid of the old one first!
 